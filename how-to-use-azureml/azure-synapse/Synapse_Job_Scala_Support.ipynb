{
 "cells": [
  {
   "source": [
    "## Get AML workspace which has synapse spark pool attached"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment, Dataset, Environment\n",
    "ws = Workspace.get(name='ws-dask-feli1',\n",
    "                   subscription_id='1aefdc5e-3a7c-4d71-a9f9-f5d3b03be19a',\n",
    "                   resource_group='feli1daskrg')\n",
    "ws"
   ]
  },
  {
   "source": [
    "## Leverage ScriptRunConfig to submit scala job to an attached synapse spark cluster"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "import uuid\n",
    "\n",
    "run_config = RunConfiguration(framework=\"pyspark\")\n",
    "run_config.target = \"link-pool\"\n",
    "run_config.spark.configuration[\"spark.driver.memory\"] = \"2g\"\n",
    "run_config.spark.configuration[\"spark.driver.cores\"] = 2\n",
    "run_config.spark.configuration[\"spark.executor.memory\"] = \"2g\"\n",
    "run_config.spark.configuration[\"spark.executor.cores\"] = 1\n",
    "run_config.spark.configuration[\"spark.executor.instances\"] = 1\n",
    "run_config.spark.configuration[\"spark.yarn.dist.jars\"]=\"abfss://testfile@feli1devstoragegen2.dfs.core.windows.net/synapse/workspaces/feli1synapsews/batchjobs/testzipfile/scalaproj_2.11-0.1.jar\"  # this can be removed if you are using local jars in source folder\n",
    "\n",
    "dir_name = \"multi-{}\".format(str(uuid.uuid4()))\n",
    "input_1 = \"abfss://testfile@feli1devstoragegen2.dfs.core.windows.net/synapse/workspaces/feli1synapsews/batchjobs/testmulti/shakespeare.txt\"\n",
    "output = \"abfss://testfile@feli1devstoragegen2.dfs.core.windows.net/synapse/workspaces/feli1synapsews/batchjobs/{}/result\".format(dir_name)\n",
    "\n",
    "from azureml.core import ScriptRunConfig\n",
    "args = ['--input', input_1, '--output', output]\n",
    "script_run_config = ScriptRunConfig(source_directory = '.',\n",
    "                                    script= 'start_script.py',\n",
    "                                    arguments= args,\n",
    "                                    run_config = run_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "exp = Experiment(workspace=ws, name='synapse-spark')\n",
    "run = exp.submit(config=script_run_config)\n",
    "run"
   ]
  },
  {
   "source": [
    "## Leverage SynapseSparkStep in an AML pipeline to add dataprep step on synapse spark cluster"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {}\n",
    "configs[\"spark.yarn.dist.jars\"] = \"abfss://testfile@feli1devstoragegen2.dfs.core.windows.net/synapse/workspaces/feli1synapsews/batchjobs/testzipfile/scalaproj_2.11-0.1.jar\"\n",
    "step_1 = SynapseSparkStep(name = 'synapse-spark',\n",
    "                          file = 'start_script.py',\n",
    "                          source_directory=\".\",\n",
    "                          arguments = args,\n",
    "                          compute_target = 'link-pool',\n",
    "                          driver_memory = \"2g\",\n",
    "                          driver_cores = 2,\n",
    "                          executor_memory = \"2g\",\n",
    "                          executor_cores = 1,\n",
    "                          num_executors = 1,\n",
    "                          conf = configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[step_1])\n",
    "pipeline_run = pipeline.submit('synapse-pipeline', regenerate_outputs=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python365jvsc74a57bd090a33703cb9cb1de13e5521056cb932021944588b79291474a45ec6e345a898a",
   "display_name": "Python 3.6.5 64-bit ('sparkcosmos': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}